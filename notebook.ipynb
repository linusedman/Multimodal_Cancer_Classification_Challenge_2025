{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d79213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import MultiModalCellDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92a9bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define transforms ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de027a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patients in dataset: 12\n",
      "Number of patients selected: 12\n",
      "Final train set: 1200 images\n",
      "Final val set:   120 images\n"
     ]
    }
   ],
   "source": [
    "# --- Config ---\n",
    "csv_path = 'multimodal-cancer-classification-challenge-2025/train.csv'\n",
    "bf_train_dir = 'multimodal-cancer-classification-challenge-2025/BF/train'\n",
    "fl_train_dir = 'multimodal-cancer-classification-challenge-2025/FL/train'\n",
    "modality = 'both'  # Choose from: 'BF', 'FL', or 'both'\n",
    "\n",
    "images_per_patient_train = 100   # Number of images to sample for training per patient\n",
    "max_patients = 12             # Set to int (e.g., 6) to limit, or None to use all\n",
    "\n",
    "# --- Load and prepare ---\n",
    "df = pd.read_csv(csv_path)\n",
    "df['patient_id'] = df['Name'].apply(lambda x: x.split('_image_')[0])\n",
    "unique_patients = df['patient_id'].unique()\n",
    "\n",
    "print(f\"Total patients in dataset: {len(unique_patients)}\")\n",
    "\n",
    "# --- Limit number of patients if needed ---\n",
    "if max_patients is not None and max_patients < len(unique_patients):\n",
    "    selected_patients = np.random.choice(unique_patients, size=max_patients, replace=False)\n",
    "else:\n",
    "    selected_patients = unique_patients\n",
    "\n",
    "print(f\"Number of patients selected: {len(selected_patients)}\")\n",
    "\n",
    "# --- Sample per patient ---\n",
    "train_rows = []\n",
    "val_rows = []\n",
    "\n",
    "for pid in selected_patients:\n",
    "    patient_df = df[df['patient_id'] == pid]\n",
    "    \n",
    "    # Determine how many images are available\n",
    "    total_images = len(patient_df)\n",
    "    train_n = min(images_per_patient_train, total_images)\n",
    "    val_n = max(1, train_n // 10)  # at least 1 for validation\n",
    "    \n",
    "    # Shuffle and split\n",
    "    sampled = patient_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    train_sample = sampled.iloc[:train_n]\n",
    "    val_sample = sampled.iloc[train_n:train_n + val_n]\n",
    "    \n",
    "    train_rows.append(train_sample)\n",
    "    val_rows.append(val_sample)\n",
    "\n",
    "# --- Concatenate and save ---\n",
    "train_df = pd.concat(train_rows).reset_index(drop=True)\n",
    "val_df = pd.concat(val_rows).reset_index(drop=True)\n",
    "\n",
    "print(f\"Final train set: {len(train_df)} images\")\n",
    "print(f\"Final val set:   {len(val_df)} images\")\n",
    "\n",
    "train_df.to_csv('sampled_train.csv', index=False)\n",
    "val_df.to_csv('sampled_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9af75ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultiModalCellDataset(\n",
    "    csv_file='sampled_train.csv',\n",
    "    bf_dir=bf_train_dir,\n",
    "    fl_dir=fl_train_dir,\n",
    "    transform=transform,\n",
    "    mode=modality\n",
    ")\n",
    "\n",
    "val_dataset = MultiModalCellDataset(\n",
    "    csv_file='sampled_val.csv',\n",
    "    bf_dir=bf_train_dir,\n",
    "    fl_dir=fl_train_dir,\n",
    "    transform=transform,\n",
    "    mode=modality\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "532a9fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n",
      "Train batch: torch.Size([16, 6, 128, 128]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# --- Sample test run ---\n",
    "if __name__ == \"__main__\":\n",
    "    for images, labels in train_loader:\n",
    "        print(\"Train batch:\", images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5da7b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modified_resnet18(input_channels=3, pretrained=True):\n",
    "    model = resnet18(pretrained=pretrained)\n",
    "\n",
    "    if input_channels != 3:\n",
    "        # Replace the first conv layer to accept more channels (e.g. 6 for BF+FL)\n",
    "        model.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "    # Replace final FC for binary classification\n",
    "    model.fc = nn.Linear(model.fc.in_features, 1)  # Output: raw logits\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64c2151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Training phase ---\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().to(device).unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            probs = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "            pred_classes = [1 if p > 0.5 else 0 for p in probs]\n",
    "            train_preds.extend(pred_classes)\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # --- Validation phase ---\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_preds = []\n",
    "        val_probs = []\n",
    "        val_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.float().to(device).unsqueeze(1)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "                pred_classes = [1 if p > 0.5 else 0 for p in probs]\n",
    "                val_probs.extend(probs)\n",
    "                val_preds.extend(pred_classes)\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_auc = roc_auc_score(val_labels, val_probs)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        # --- Output ---\n",
    "        print(f\"Epoch {epoch+1:03d} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a1e3e056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linus/Library/CloudStorage/OneDrive-Uppsalauniversitet/Avancerad djupinlärning/.venv/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/linus/Library/CloudStorage/OneDrive-Uppsalauniversitet/Avancerad djupinlärning/.venv/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "if modality.lower() == 'both':\n",
    "    input_channels = 6\n",
    "else:\n",
    "    input_channels = 3\n",
    "\n",
    "model = get_modified_resnet18(input_channels=input_channels ,pretrained=True)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a3d983f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Loss: 0.4805, Acc: 0.7742 | Val Loss: 0.4312, Acc: 0.7417, AUC: 0.8760\n",
      "Epoch 002 | Train Loss: 0.3071, Acc: 0.8742 | Val Loss: 1.6148, Acc: 0.4833, AUC: 0.7329\n",
      "Epoch 003 | Train Loss: 0.1950, Acc: 0.9200 | Val Loss: 0.2577, Acc: 0.8750, AUC: 0.9583\n",
      "Epoch 004 | Train Loss: 0.1578, Acc: 0.9308 | Val Loss: 0.2551, Acc: 0.9000, AUC: 0.9557\n",
      "Epoch 005 | Train Loss: 0.0846, Acc: 0.9650 | Val Loss: 0.2417, Acc: 0.8750, AUC: 0.9663\n",
      "Epoch 006 | Train Loss: 0.1423, Acc: 0.9483 | Val Loss: 0.3826, Acc: 0.8500, AUC: 0.9429\n",
      "Epoch 007 | Train Loss: 0.0943, Acc: 0.9650 | Val Loss: 0.1944, Acc: 0.8750, AUC: 0.9714\n",
      "Epoch 008 | Train Loss: 0.0646, Acc: 0.9775 | Val Loss: 0.4688, Acc: 0.8250, AUC: 0.9549\n",
      "Epoch 009 | Train Loss: 0.0632, Acc: 0.9775 | Val Loss: 0.8402, Acc: 0.8500, AUC: 0.9266\n",
      "Epoch 010 | Train Loss: 0.0734, Acc: 0.9767 | Val Loss: 0.3723, Acc: 0.8417, AUC: 0.9346\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
